[![License](https://img.shields.io/badge/License-GNU%20GPLv3-green)](./LICENSE) [![DOI](https://zenodo.org/badge/349075288.svg)](https://zenodo.org/record/7252232#.Y7yNv6fMJhG) [![DOI](https://img.shields.io/badge/DOI-10.1145%2F3313828%20-orange)](https://doi.org/10.48550/arXiv.2210.13495) [![DOI](https://img.shields.io/badge/DOI-10.1002%2Fcpe.3394%20-orange)](http://dx.doi.org/10.23919/mipro55190.2022.9803591)

# Entanglement cooling algorithm

The **Entanglement cooling algorithm** calculates the statistical properties of the entanglement spectrum by applying a Metropolis-like entanglement cooling algorithm, generated by different sets of local gates, on states sharing the same statistic. The initial states are the ground states of the one-dimensional quantum Ising model in its different macroscopic phases (paramagnetic, magnetically ordered and  topologically frustrated).

The code is written in Python, which allows for straightforward portability across different computing platforms. The code supports execution on shared memory systems (manycore and multi-CPU systems) and on distributed GPU systems using the Message Passing Interface (MPI).

The code was developed at the [Ruđer Bošković Institute](https://www.irb.hr/) by the Condensed Matter and Statistical Physics Group and the [Centre for Informatics and Computing](https://cir.com.hr/) and supported by the Croatia Science Foundation through project UIP-2020-02-4559 "Scalable High-Performance Algorithms for Future Heterogeneous Distributed Computing Systems"([HybridScale](https://www.croris.hr/projekti/projekt/6243?lang=en)).

## Dependencies

- conda
- numpy
- scipy
- mpi4py (for distributed support)
- cupy (for GPU versions)
- argparse
- scipy
- mpi4py
- cupy
- nvtx
- itertools
- pickle

A detailed list of the required Python libraries and dependencies is given in the file [Requirements.txt](./Requirements.txt).

## Versions of the code

The Entaglement cooling algorithm supports three execution modes depending on the available hardware resources:

- **CPU:** The code is executed on CPUs using MPI.

- **GPU:** The code is executed on GPUs using MPI. Only one Monte Carlo simulation is run per GPU. If multiple Monte Carlo simulations are to be run, they are run in a sequential order.

- **batchedGEMM:** The code is executed on (distributed) GPUs using MPI. Multiple Monte Carlo simulations are packed into one larger one and executed simultaneously on the GPU. This approach achieves much higher performance than the previous two modes.

## Quick start

### Cloning the code

```bash
git clone https://github.com/HybridScale/Entanglement-Cooling-Algorithm.git
```

### Install Python environment

The repository provides the file [Requirements.txt](./Requirements.txt) with Python packages needed for simulation. To create new Virtual Envirnoment in Python with Conda and install all required packages run:

```bash
conda create --name myenv --file Requirements.txt
```

To activate conda environment with all the required dependencies installed run:
```bash
conda activate myenv
```

### Running

The repository provides the script [src/main.py](main.py), which implements the CLI interface.  
To get information about different code versions, run:

```bash
python src/main.py -h
``` 
There are three different versions available: `CPU`, `GPU` and `BatchedGEMM`.
 
Both `GPU` and `BatchedGEMM` support execution on GPUs. With `GPU`, one Monte Carlo simulation is executed per GPU and with `batchedGEMM`, multiple simulations are combined in a batch operation and executed simulatenously on a single GPU.

The simulation can be resumed from the last simulation with the same initial parameters or a new simulation can be started.
To check the options, execute:

```
python src/main.phy {CPU,GPU,batchedGEMM} -h
```

Use positional arguments to select the desired version. CLI also provides information about all available command line options and parameters.

```
python src/main.phy {CPU, GPU, batchedGEMM} {new, resume} -h
```

## Examples
In all the following examples we will use the same simulation parameters: `19` lattice sites, subsystem of size `9`, coupling parameter `2.5`, `1000` Monte Carlo simulations steps with `10000000` steps to be finally calculated.

### CPU version
Starting a simulation takes up all processor resources.
```bash
python src/main CPU new --N 19 --R 9 --L 2.5 --MC 1000 -w 10000000
```
Multiple simulation can be calculated simultaneously using `MPI` to start `N` simulations, but each `MPI` process would compete for resources (CPU cores). To overcome this problem, set the number of cores that can be used per simulation. If you have 96 cores and want to run 8 simulations, you must set the environment variable 'OMP_NUM_ THREADS' to 12. 

```bash
export OMP_NUM_THREADS=12
mpirun -n 8 python src/main CPU new --N 19 --R 9 --L 2.5 --MC 1000 -w 10000000
```

### GPU version
Starting a simulation computed on a single graphics card:
'''bash mpirun -n 4 python src/main.py GPU new --N 19 --R 9 --L 2.5 -- MC 100 -w 10000000
'''
Multiple simulations can be computed simultaneously using 'MPI' to run "N' simulations, distributing the "MPI' process across multiple GPUs present in the system. 12 simulations on a system with 4 GPUs would calculate 3 simulations per GP and each simulation creates personal process context effectively leads to competition for resources. To overcome this problem, start NVIDIA Multi-Process Service ([MPS](https://docs.nvidia.com/deploy/mps/index.html)) before running simulations and after executing, shut it down:
```bash
nvidia-cuda-mps-control -d

sleep 10

mpirun -n 4 python src/main.py GPU new --N 19 --R 9 --L 2.5 --MC 100 -w 10000000
echo quit | nvidia-cuda-mps-control
```

### batchedGEMM version
For batchedGEMM version number of simulations calculated on single GPU is set with parsing argument `--bs`. For 8 simulations:
```bash
python src/main.py batchedGEMM new --N 19 --R 9 --L 2.5 --MC 100 -w 10000000 --bs 8
```
There is an upper limit to the number of simulations that can be run on a single graphics card, so it is possible to  run multiple batch GEMM versions simultaneously. It is recommended to run a single batchedGEMM version on a GPU. In the next example, 4 'batchedGEMM' are run on a system with 4 GPUs, with each 'batchedGEMM' running 8 simulations, for a total of 32 simulations:

```bash
mpirun -n 4 python src/main.py batchedGEMM new --N 19 --R 9 --L 2.5 --MC 100 -w 10000000 --bs 8

```

## Fine-tune the execution

To be done

## Developers

### The main code

- Jovan Odavić 
- Gianpaolo Torre
- Fabio Franchini
- Salvatore Marco Gaimpaolo

### GPU parallelisation and code optimisation

- Davor Davidović
- Nenad Mijić

## Contribution

This repository mirrors the principal repository of the code on Bitbucket. If you want to contribute to the code please contact *davor.davidovic@irb.hr* or *jovan.odavic@irb.hr*.

## How to cite the code

Description of the computational model and the entanglement cooling with Metropolis Monte Carlo:

J. Odavić, G. Torre, N. Mijić, D. Davidović, F. Franchini, S. M. Giampaolo.  *Random unitaries, Robustness, and Complexity of Entanglement*. [arXiv:2210.13495](https://doi.org/10.48550/arXiv.2210.13495)

The code and parallelisation on distributed multi-GPU computing architectures:

N. Mijic, D. Davidovic. *Batched matrix operations on distributed GPUs with application in theoretical physics.* 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO), 2022, pp. 293-299, [doi: 10.23919/MIPRO55190.2022.9803591](http://dx.doi.org/10.23919/mipro55190.2022.9803591).


The full paper is available [here](http://fulir.irb.hr/7514/).

## Copyright and License

This code is published under GNU General Public License v3.0 ([GNU GPLv3](./LICENSE))
