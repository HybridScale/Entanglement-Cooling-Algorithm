[![License](https://img.shields.io/badge/License-GNU%20GPLv3-green)](./LICENSE) [![DOI](https://zenodo.org/badge/349075288.svg)](https://zenodo.org/record/7252232#.Y7yNv6fMJhG) [![DOI](https://img.shields.io/badge/DOI-10.1145%2F3313828%20-orange)](https://doi.org/10.48550/arXiv.2210.13495) [![DOI](https://img.shields.io/badge/DOI-10.1002%2Fcpe.3394%20-orange)](http://dx.doi.org/10.23919/mipro55190.2022.9803591)

# Entanglement cooling algorithm

The **Entanglement cooling algorithm** calculates the statistical properties of the entanglement spectrum by applying a Metropolis-like entanglement cooling algorithm, generated by different sets of local gates, on states sharing the same statistic. The initial states are the ground states of the one-dimensional quantum Ising model in its different macroscopic phases (paramagnetic, magnetically ordered and  topologically frustrated).

The code is written in Python, which allows for straightforward portability across different computing platforms. The code supports execution on shared memory systems (manycore and multi-CPU systems) and on distributed GPU systems using the Message Passing Interface (MPI).

The code was developed at the [Ruđer Bošković Institute](https://www.irb.hr/) by the Condensed Matter and Statistical Physics Group and the [Centre for Informatics and Computing](https://cir.com.hr/) and supported by the Croatia Science Foundation through project UIP-2020-02-4559 "Scalable High-Performance Algorithms for Future Heterogeneous Distributed Computing Systems"([HybridScale](https://www.croris.hr/projekti/projekt/6243?lang=en)).

## Dependencies

- conda
- numpy
- scipy
- mpi4py (for distributed support)
- cupy (for GPU versions)
- argparse
- scipy
- mpi4py
- cupy
- nvtx
- itertools
- pickle

A detailed list of the required Python libraries and dependencies is given in the file [Requirements.txt](./Requirements.txt).

## Versions of the code

The Entaglement cooling algorithm supports three execution modes depending on the available hardware resources:

- **CPU:** The code is executed on CPUs using MPI.

- **GPU:** The code is executed on GPUs using MPI. Only one Monte Carlo simulation is run per GPU. If multiple Monte Carlo simulations are to be run, they are run in a sequential order.

- **batchedGEMM:** The code is executed on (distributed) GPUs using MPI. Multiple Monte Carlo simulations are packed into one larger one and executed simultaneously on the GPU. This approach achieves much higher performance than the previous two modes.

## Quick start

### Cloning the code

```bash
git clone https://github.com/HybridScale/Entanglement-Cooling-Algorithm.git
```

### Install Python environment

The repository provides the file [Requirements.txt](./Requirements.txt) with Python packages needed for simulation. To create new Virtual Envirnoment in Python with Conda and install all required packages run:

```bash
conda create --name myenv --file Requirements.txt
```

To activate conda environment with all the required dependencies installed run:
```bash
conda activate myenv
```

### Running

The repository provides the script [src/main.py](main.py), which implements the CLI interface.  
To get information about different code versions, run:

```bash
python src/main.py -h
``` 
There are three different versions available: `CPU`, `GPU` and `BatchedGEMM`.
 
Both `GPU` and `BatchedGEMM` support execution on GPUs. With `GPU`, one Monte Carlo simulation is executed per GPU and with `batchedGEMM`, multiple simulations are combined in a batch operation and executed simulatenously on a single GPU.

The simulation can be resumed from the last simulation with the same initial parameters or a new simulation can be started.
To check the options, execute:

```
python src/main.phy {CPU,GPU,batchedGEMM} -h
```

Use positional arguments to select the desired version. CLI also provides information about all available command line options and parameters.

```
python src/main.phy {CPU, GPU, batchedGEMM} {new, resume} -h
```

### Run multiple simulations in parallel
By using `MPI`, several simulations (Monte Carlo simulations) can be calculated at the same time. 

An example: start a new run on `4` MPI processes (ranks) with `19` lattice sites, subsystem of size `9`, coupling parameter `2.5`, `100` Monte Carlo simulations with `10000000` steps each.

```bash
mpirun -n 4 python src/main.py GPU new --N 19 --R 9 --L 2.5 --MC 100 -w 10000000
```

### Examples

To be done

## Fine-tune the execution

To be done

## Developers

### The main code

- Jovan Odavić 
- Gianpaolo Torre
- Fabio Franchini
- Salvatore Marco Gaimpaolo

### GPU parallelisation and code optimisation

- Davor Davidović
- Nenad Mijić

## Contribution

This repository mirrors the principal repository of the code on Bitbucket. If you want to contribute to the code please contact *davor.davidovic@irb.hr* or *jovan.odavic@irb.hr*.

## How to cite the code

Description of the computational model and the entanglement cooling with Metropolis Monte Carlo:

J. Odavić, G. Torre, N. Mijić, D. Davidović, F. Franchini, S. M. Giampaolo.  *Random unitaries, Robustness, and Complexity of Entanglement*. [arXiv:2210.13495](https://doi.org/10.48550/arXiv.2210.13495)

The code and parallelisation on distributed multi-GPU computing architectures:

N. Mijic, D. Davidovic. *Batched matrix operations on distributed GPUs with application in theoretical physics.* 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO), 2022, pp. 293-299, [doi: 10.23919/MIPRO55190.2022.9803591](http://dx.doi.org/10.23919/mipro55190.2022.9803591).


The full paper is available [here](http://fulir.irb.hr/7514/).

## Copyright and License

This code is published under GNU General Public License v3.0 ([GNU GPLv3](./LICENSE))
